{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The goal of this assignment is to create a basic program that provides an overview of basic evaluation metrics (in particular, precision, recall, f-score and a confusion matrix) from documents provided in the conll format. \n",
    "You will need to implement the calculations for precision, recall and f-score yourself (i.e. do not use an existing module that spits them out). Make sure that your code can handle the situation where there are no true positives for a specific class.\n",
    "\n",
    "This notebook provides functions for reading in conll structures with pandas and proposes a structure for calculating your evaluation metrics and producing the confusion matrix. Feel free to adjust the proposed structure if you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "# see tips & tricks on using defaultdict (remove when you do not use it)\n",
    "from collections import defaultdict, Counter\n",
    "# module for verifying output\n",
    "from nose.tools import assert_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note Pandas\n",
    "\n",
    "Pandas is a module that provides data structures and is widely used for dealing with data representations in machine learning. It is a bit more advanced than the csv module we saw in the preprocessing notebook.\n",
    "Working with pandas data structures can be tricky, but it will generally work well if you follow online tutorials and examples closely. If your code is slow before you even started training your models, it is likely to be a problem with the way you are using Pandas (it will still work in most cases, you will just have to wait a bit longer). Once you are more used to working with modules and complex objects, it will also become easier to work with Pandas.\n",
    "\n",
    "In the examples below, we assume that the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotations(inputfile, annotationcolumn, delimiter='\\t'):\n",
    "    '''\n",
    "    This function extracts annotations represented in the conll format from a file\n",
    "    \n",
    "    :param inputfile: the path to the conll file\n",
    "    :param annotationcolumn: the name of the column in which the target annotation is provided\n",
    "    :param delimiter: optional parameter to overwrite the default delimiter (tab)\n",
    "    :type inputfile: string\n",
    "    :type annotationcolumn: string\n",
    "    :type delimiter: string\n",
    "    :returns: the annotations as a list\n",
    "    '''\n",
    "    #https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    conll_input = pd.read_csv(inputfile, sep=delimiter)\n",
    "    annotations = conll_input[annotationcolumn].tolist()\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_counts(goldannotations, machineannotations):\n",
    "    '''\n",
    "    This function compares the gold annotations to machine output\n",
    "    \n",
    "    :param goldannotations: the gold annotations\n",
    "    :param machineannotations: the output annotations of the system in question\n",
    "    :type goldannotations: the type of the object created in extract_annotations\n",
    "    :type machineannotations: the type of the object created in extract_annotations\n",
    "    \n",
    "    :returns: a countainer providing the counts for each predicted and gold class pair\n",
    "    '''\n",
    "    \n",
    "    # TIP on how to get the counts for each class\n",
    "    # https://stackoverflow.com/questions/49393683/how-to-count-items-in-a-nested-dictionary, last accessed 22.10.2020\n",
    "    evaluation_counts = defaultdict(Counter)\n",
    "    assert len(goldannotations) == len(machineannotations)\n",
    "    for i in range(len(goldannotations)):\n",
    "        evaluation_counts[goldannotations[i]][machineannotations[i]] += 1\n",
    "    return evaluation_counts\n",
    "    \n",
    "def calculate_precision_recall_fscore(evaluation_counts):\n",
    "    '''\n",
    "    Calculate precision recall and fscore for each class and return them in a dictionary\n",
    "    \n",
    "    :param evaluation_counts: a container from which you can obtain the true positives, false positives and false negatives for each class\n",
    "    :type evaluation_counts: type of object returned by obtain_counts\n",
    "    \n",
    "    :returns the precision, recall and f-score of each class in a container\n",
    "    '''\n",
    "    \n",
    "    # TIP: you may want to write a separate function that provides an overview of true positives, false positives and false negatives\n",
    "    #      for each class based on the outcome of obtain counts\n",
    "    # YOUR CODE HERE (and remove statement below)\n",
    "    value_dict = defaultdict(Counter)\n",
    "\n",
    "    \n",
    "    for label in evaluation_counts.keys():\n",
    "        # Calculation of TP, FP & FN\n",
    "        TP = evaluation_counts[label][label]\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        for label_ in evaluation_counts.keys():\n",
    "            if label_ != label:\n",
    "                FP += evaluation_counts[label_][label]\n",
    "                FN += evaluation_counts[label][label_]\n",
    "\n",
    "        # Precision, Recall & F-Score for class 'label'\n",
    "        try:\n",
    "            precis = TP / (TP + FP)\n",
    "        except ZeroDivisionError:\n",
    "            precis = 0\n",
    "        try:\n",
    "            recall = TP / (TP + FN)\n",
    "        except ZeroDivisionError:\n",
    "            recall = 0\n",
    "        try:\n",
    "            FScore = (2 * precis * recall) / (precis + recall)\n",
    "        except ZeroDivisionError:\n",
    "            FScore = 0\n",
    "        \n",
    "        # Assign Precision, Recall, & F-Score according to set structure\n",
    "        value_dict[label]['precision'] = precis\n",
    "        value_dict[label]['recall'] = recall\n",
    "        value_dict[label]['f-score'] = FScore\n",
    "\n",
    "    return value_dict\n",
    "            \n",
    "\n",
    "def provide_confusion_matrix(evaluation_counts):\n",
    "    '''\n",
    "    Read in the evaluation counts and provide a confusion matrix for each class\n",
    "    \n",
    "    :param evaluation_counts: a container from which you can obtain the true positives, false positives and false negatives for each class\n",
    "    :type evaluation_counts: type of object returned by obtain_counts\n",
    "    \n",
    "    :prints out a confusion matrix\n",
    "    '''\n",
    "    \n",
    "    # TIP: provide_output_tables does something similar, but those tables are assuming one additional nested layer\n",
    "    #      your solution can thus be a simpler version of the one provided in provide_output_tables below\n",
    "    \n",
    "    # YOUR CODE HERE (and remove statement below)\n",
    "    res = pd.DataFrame.from_dict(evaluation_counts).fillna(0)\n",
    "\n",
    "    print(res)\n",
    "    print(res.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carry_out_evaluation(gold_annotations, systemfile, systemcolumn, delimiter='\\t'):\n",
    "    '''\n",
    "    Carries out the evaluation process (from input file to calculating relevant scores)\n",
    "    \n",
    "    :param gold_annotations: list of gold annotations\n",
    "    :param systemfile: path to file with system output\n",
    "    :param systemcolumn: indication of column with relevant information\n",
    "    :param delimiter: specification of formatting of file (default delimiter set to '\\t')\n",
    "    \n",
    "    returns evaluation information for this specific system\n",
    "    '''\n",
    "    system_annotations = extract_annotations(systemfile, systemcolumn, delimiter)\n",
    "    evaluation_counts = obtain_counts(gold_annotations, system_annotations)\n",
    "    provide_confusion_matrix(evaluation_counts)\n",
    "    evaluation_outcome = calculate_precision_recall_fscore(evaluation_counts)\n",
    "    \n",
    "    return evaluation_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_output_tables(evaluations):\n",
    "    '''\n",
    "    Create tables based on the evaluation of various systems\n",
    "    \n",
    "    :param evaluations: the outcome of evaluating one or more systems\n",
    "    '''\n",
    "    #https:stackoverflow.com/questions/13575090/construct-pandas-dataframe-from-items-in-nested-dictionary\n",
    "    evaluations_pddf = pd.DataFrame.from_dict({(i,j): evaluations[i][j]\n",
    "                                              for i in evaluations.keys()\n",
    "                                              for j in evaluations[i].keys()},\n",
    "                                             orient='index')\n",
    "    print(evaluations_pddf)\n",
    "    print(evaluations_pddf.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluations(goldfile, goldcolumn, systems):\n",
    "    '''\n",
    "    Carry out standard evaluation for one or more system outputs\n",
    "    \n",
    "    :param goldfile: path to file with goldstandard\n",
    "    :param goldcolumn: indicator of column in gold file where gold labels can be found\n",
    "    :param systems: required information to find and process system output\n",
    "    :type goldfile: string\n",
    "    :type goldcolumn: integer\n",
    "    :type systems: list (providing file name, information on tab with system output and system name for each element)\n",
    "    \n",
    "    :returns the evaluations for all systems\n",
    "    '''\n",
    "    evaluations = {}\n",
    "    #not specifying delimiters here, since it corresponds to the default ('\\t')\n",
    "    gold_annotations = extract_annotations(goldfile, goldcolumn)\n",
    "    for system in systems:\n",
    "        sys_evaluation = carry_out_evaluation(gold_annotations, system[0], system[1])\n",
    "        evaluations[system[2]] = sys_evaluation\n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the overall set-up\n",
    "\n",
    "The functions below illustrate how to run the setup as outlined above using a main function and, later, commandline arguments. This setup will facilitate the transformation to an experimental setup that no longer makes use of notebooks, that you will submit later on. There are also some functions that can be used to test your implementation You can carry out a few small tests yourself with the data provided in the data/ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_evaluation_value(system, class_label, value_name, evaluations):\n",
    "    '''\n",
    "    Return the outcome of a specific value of the evaluation\n",
    "    \n",
    "    :param system: the name of the system\n",
    "    :param class_label: the name of the class for which the value should be returned\n",
    "    :param value_name: the name of the score that is returned\n",
    "    :param evaluations: the overview of evaluations\n",
    "    \n",
    "    :returns the requested value\n",
    "    '''\n",
    "    return evaluations[system][class_label][value_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_information(system_information):\n",
    "    '''\n",
    "    Takes system information in the form that it is passed on through sys.argv or via a settingsfile\n",
    "    and returns a list of elements specifying all the needed information on each system output file to carry out the evaluation.\n",
    "    \n",
    "    :param system_information is the input as from a commandline or an input file\n",
    "    '''\n",
    "    # https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "    systems_list = [system_information[i:i + 3] for i in range(0, len(system_information), 3)]\n",
    "    return systems_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(my_args=None):\n",
    "    \n",
    "    if my_args is None:\n",
    "        my_args = sys.argv\n",
    "    \n",
    "    system_info = create_system_information(my_args[2:])\n",
    "    evaluations = run_evaluations(my_args[0], my_args[1], system_info)\n",
    "    provide_output_tables(evaluations)\n",
    "    check_eval = identify_evaluation_value('system1', 'O', 'f-score', evaluations)\n",
    "    #if it does not work, this assert stateme\n",
    "    assert_equal(\"%.3f\" % check_eval,\"0.889\")\n",
    "    \n",
    "\n",
    "# these can come from the commandline using sys.argv for instance\n",
    "my_args = ['../../data/minigold.csv','gold','../../data/miniout1.csv','NER','system1']\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some additional tests\n",
    "\n",
    "test_args = ['../../data/minigold.csv','gold','../../data/miniout2.csv','NER','system2']\n",
    "system_info = create_system_information(test_args[2:])\n",
    "evaluations = run_evaluations(test_args[0], test_args[1], system_info)\n",
    "test_eval = identify_evaluation_value('system2', 'I-ORG', 'f-score', evaluations)\n",
    "assert_equal(\"%.3f\" % test_eval,\"0.571\")\n",
    "test_eval2 = identify_evaluation_value('system2', 'I-PER', 'precision', evaluations)\n",
    "assert_equal(\"%.3f\" % test_eval2,\"0.500\")\n",
    "test_eval3 = identify_evaluation_value('system2', 'I-ORG', 'recall', evaluations)\n",
    "assert_equal(\"%.3f\" % test_eval3,\"0.667\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}