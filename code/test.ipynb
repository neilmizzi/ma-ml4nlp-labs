{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "lan_model = gensim.models.KeyedVectors.load_word2vec_format('D:/Projects/ma-ml4nlp-labs//models/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NER_ML.py\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "import sys\n",
    "from eval import compare_outcome, get_macro_score\n",
    "\n",
    "\n",
    "class NERML:\n",
    "    # INIT Module\n",
    "    # \n",
    "    # takes train & test dirs and loads DataFrames\n",
    "    # Also takes boolean, typeset to False, determining whether to load embeddings or not\n",
    "    # \n",
    "    # Sets train and test DFs, loads Vectoriser, and loads word embeddings (if enabled)\n",
    "    def __init__(self, train: str, test: str, load_embeddings, iter_lim:int = 15000) -> None:\n",
    "        # Set Iteration Limit to be used by SVM & NB (and possibly other models that need early stopping)\n",
    "        self.iter_lim = iter_lim\n",
    "\n",
    "        # List of all features loaded\n",
    "        self.feature_list = ['token', 'ChunkLabel', 'POS-Tag', 'PrevToken', \n",
    "        'NextToken', 'FULLCAPS', 'FirstCaps', 'NERLabel']\n",
    "\n",
    "        # Load train and test files as Pandas DF\n",
    "        # Names are set in case files do not have headers. Remove them if they do.\n",
    "        self.train_file : DataFrame = pd.read_csv(train, sep='\\t', \n",
    "        names=self.feature_list)\n",
    "        self.train_file.columns = self.feature_list\n",
    "        self.train_file = self.train_file.fillna(0)\n",
    "        self.test_file : DataFrame = pd.read_csv(test, sep='\\t', \n",
    "        names=self.feature_list)\n",
    "        self.test_file.columns = self.feature_list\n",
    "        self.test_file = self.test_file.fillna(0)\n",
    "\n",
    "        # Set Dict Vectorizer\n",
    "        self.vec = DictVectorizer()\n",
    "\n",
    "        # loads Google Word2Vec Word Embeddings (Make sure directory matches)\n",
    "        # only done if embeddings will be used\n",
    "        # This process takes its time if set to true\n",
    "        self.embeddings_loaded : bool = False\n",
    "        if load_embeddings:\n",
    "            # Possible future update: Make this snippet a separate function\n",
    "            # Allowing us to load the word embeddings after initialisation of NER class\n",
    "            # But also retaining the option of doing it within the init process\n",
    "            self.language_model = load_embeddings\n",
    "            self.embeddings_loaded = True\n",
    "\n",
    "\n",
    "    # determines whether to work with train or test based on bool\n",
    "    def train_or_test_getter(self, is_train:bool) -> DataFrame:\n",
    "        return self.train_file if is_train else self.test_file\n",
    "\n",
    "\n",
    "    # Sets the vectorised embeddings to the DF\n",
    "    def set_embeddings(self, is_train:bool, feat:str) -> None:\n",
    "        if self.embeddings_loaded:\n",
    "            features = []\n",
    "            df = self.train_or_test_getter(is_train)\n",
    "            tokens = df[feat].to_list()\n",
    "\n",
    "            # code obtained from provided files\n",
    "            for token in tokens:\n",
    "                if token in self.language_model:\n",
    "                    vector = self.language_model[token]\n",
    "                else:\n",
    "                    vector = [0]*300\n",
    "                features.append(vector)\n",
    "            if is_train:\n",
    "                self.train_file['embeddings_'+feat] = features\n",
    "            else:\n",
    "                self.test_file['embeddings_'+feat] = features \n",
    "        else:\n",
    "            raise FileNotFoundError(\"Embeddings not Loaded!\")\n",
    "    \n",
    "\n",
    "    # Get Embeddings as list\n",
    "    def get_embeddings(self, is_train:bool, feat:str) -> list:\n",
    "        self.set_embeddings(is_train, feat)\n",
    "        if is_train:\n",
    "            return self.train_file['embeddings_'+feat].to_list()\n",
    "        else:\n",
    "            return self.test_file['embeddings_'+feat].to_list()\n",
    "    \n",
    "    \n",
    "    # Returns Vectorised Dict Feats for either Train or Test\n",
    "    def get_feat_vect(self, is_train:bool, selected_features:list) -> any:\n",
    "        basic_features = selected_features[:]\n",
    "\n",
    "        # Remove any features for which we can use word embeddings from basic list... if enabled\n",
    "        if self.embeddings_loaded:\n",
    "            if 'token' in basic_features:\n",
    "                basic_features.remove('token')\n",
    "            if 'PrevToken' in basic_features:\n",
    "                basic_features.remove('PrevToken')\n",
    "        \n",
    "        # Get correct dataset\n",
    "        df = self.train_or_test_getter(is_train)\n",
    "\n",
    "        # Get basic features\n",
    "        if basic_features:\n",
    "            dict_feats = []\n",
    "            for i in range(df.shape[0]):\n",
    "                sub_dict = {}\n",
    "                for feat in basic_features:\n",
    "                    sub_dict[feat] = df.at[i, feat]\n",
    "                dict_feats.append(sub_dict)\n",
    "        \n",
    "            # Only basic features at this point\n",
    "            feats = self.vec.fit_transform(dict_feats) if is_train else self.vec.transform(dict_feats)\n",
    "        else:\n",
    "            feats = []\n",
    "\n",
    "        # add word embeddings to feature set if they are loaded, or we have features that can have word embeddings\n",
    "        if self.embeddings_loaded and ('token' in selected_features or 'PrevToken' in selected_features):\n",
    "            combined_vectors = []\n",
    "\n",
    "            # Vectorise features\n",
    "            feats = np.array(feats.toarray()) if feats != [] else np.empty([df.shape[0], 0])\n",
    "\n",
    "            # Future fix: Add var which specifies which features will be word embeddings (if enabled)\n",
    "\n",
    "            # Check if we actually have vars in our selected feature list\n",
    "            if 'token' in selected_features:\n",
    "                embeddings_token = self.get_embeddings(is_train, feat='token')\n",
    "            if 'PrevToken' in selected_features:\n",
    "                embeddings_prevTok = self.get_embeddings(is_train, feat='PrevToken')\n",
    "\n",
    "            flag_token_only = False\n",
    "            flag_token_vectorised_only = False\n",
    "            for i, vector in enumerate(feats):\n",
    "                if 'token' in selected_features:\n",
    "                    # Before concatenating, check the shape of the vectorised features\n",
    "                    # If it's 0, this indicates we have no vectorised features (i.e. no basic features)\n",
    "                    if feats.shape[1] != 0:\n",
    "                        flag_token_vectorised_only = True\n",
    "                        combined_vector = np.concatenate((vector, embeddings_token[i]))\n",
    "                        combined_vector = np.array([combined_vector])\n",
    "                    else:\n",
    "                        flag_token_only = True\n",
    "                        combined_vector = np.array([embeddings_token[i]])\n",
    "                else:\n",
    "                    combined_vector = np.array([vector])\n",
    "                if 'PrevToken' in selected_features:\n",
    "                    # We do the same check for PrevToken\n",
    "                    # If combined_vector still is empty, then we have no features (yet)\n",
    "                    # And our only feature will be PrevToken word embeddings\n",
    "                    # Else, combine with whatever we have.\n",
    "                    if combined_vector.shape[1] != 0:\n",
    "                        flag_token_vectorised_only  = False\n",
    "                        flag_token_only = False\n",
    "                        combined_vector = np.concatenate((combined_vector[0], embeddings_prevTok[i]))\n",
    "                    else:\n",
    "                        combined_vector = embeddings_prevTok[i]\n",
    "                else:\n",
    "                    pass\n",
    "                if flag_token_only or flag_token_vectorised_only:\n",
    "                    combined_vector = combined_vector[0]\n",
    "                combined_vectors.append(combined_vector)\n",
    "            feats = combined_vectors\n",
    "        \n",
    "        return feats\n",
    "\n",
    "\n",
    "    # Creates and Returns a trained model, specified by model_name. Can be LR, NB or SVM\n",
    "    def create_classifier(self, feats, model_name:str) -> any:\n",
    "        print(f\"Training {model_name}...\")\n",
    "        targets = self.train_file['NERLabel'].to_list()\n",
    "        if model_name ==  'LR':\n",
    "            model = LogisticRegression(max_iter=self.iter_lim)\n",
    "            model.fit(feats, targets)\n",
    "\n",
    "        elif model_name == 'NB':\n",
    "            model = MultinomialNB()\n",
    "            model.fit(feats, targets)\n",
    "\n",
    "        elif model_name == 'SVM':\n",
    "            model = SVC(max_iter=self.iter_lim)\n",
    "            model.fit(feats, targets)\n",
    "        else:\n",
    "            raise ValueError(\"Model is not known, or not implemented\")\n",
    "        return model\n",
    "\n",
    "\n",
    "    # Given a model, we test against the test data\n",
    "    def set_predictions(self, model, sel_feats) -> list:\n",
    "        feats = self.get_feat_vect(is_train=False, selected_features=sel_feats)\n",
    "        predictions = model.predict(feats)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    # Returns Macro Scores\n",
    "    def get_performance(self, predictions) -> any:\n",
    "        return get_macro_score(predictions, self.test_file['NERLabel'].to_list())\n",
    "\n",
    "\n",
    "    # Provides Confusion Matrix and class-by-class scores\n",
    "    def get_prediction_summary(self, predictions) -> None:\n",
    "        compare_outcome(predictions, self.test_file['NERLabel'].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running Part 1: Ablation Analysis\n",
      "Running extensive tests on one system\n",
      "Testing combination ['token', 'FirstCaps']\n",
      "Training LR...\n",
      "Testing combination ['token']\n",
      "Training LR...\n",
      "Testing combination ['POS-Tag', 'PrevToken', 'ChunkLabel']\n",
      "Training LR...\n",
      "Testing combination ['token', 'PrevToken']\n",
      "Training LR...\n",
      "Testing combination ['token', 'PrevToken', 'ChunkLabel']\n",
      "Training LR...\n"
     ]
    }
   ],
   "source": [
    "#ass3.py\n",
    "# from ner_ml import NERML\n",
    "from itertools import compress, product\n",
    "\n",
    "# obtained from https://stackoverflow.com/a/6542458/5161292\n",
    "def combinations(items):\n",
    "    sub_list = (set(compress(items, mask)) for mask in product(*[[0,1]]*len(items)))\n",
    "    return [list(x) for x in sub_list]\n",
    "\n",
    "\n",
    "# Create NER Instance\n",
    "ner = NERML('D:/Projects/ma-ml4nlp-labs/data/reuters-train-tab-stripped.en', 'D:/Projects/ma-ml4nlp-labs/data/gold_stripped.conll', lan_model)\n",
    "print(\"Running Part 1: Ablation Analysis\")\n",
    "print(\"Running extensive tests on one system\")\n",
    "\n",
    "# Selection of all features\n",
    "sel_feats = ['token', 'ChunkLabel', 'POS-Tag', 'PrevToken', 'FULLCAPS', 'FirstCaps']\n",
    "\n",
    "# Get all possible combinations of features\n",
    "feature_combs = combinations(sel_feats)\n",
    "del feature_combs[0]\n",
    "feature_combs.remove(['PrevToken'])\n",
    "\n",
    "results_dict = {'variables': [],\n",
    "                'precision': [],\n",
    "                'recall': [],\n",
    "                'f-score': []}\n",
    "\n",
    "for comb in feature_combs:\n",
    "    print(f\"Testing combination {comb}\")\n",
    "    # Get Vectorised features\n",
    "    vec_feats = ner.get_feat_vect(True, comb)\n",
    "\n",
    "    # Train on all models and get results\n",
    "    for model_name in ['LR']:\n",
    "        classifier = ner.create_classifier(vec_feats, model_name)\n",
    "        predictions = ner.set_predictions(classifier, comb)\n",
    "        precision, recall, f_score = ner.get_performance(predictions)\n",
    "\n",
    "        results_dict['variables'].append(comb)\n",
    "        results_dict['precision'].append(precision)\n",
    "        results_dict['recall'].append(recall)\n",
    "        results_dict['f-score'].append(f_score)\n",
    "\n",
    "results = pd.DataFrame().from_dict(results_dict)\n",
    "results.to_csv('D:/Projects/ma-ml4nlp-labs/data/ass3_results_ablanal_deep.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}