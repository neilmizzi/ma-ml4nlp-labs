{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "loaded_model = gensim.models.KeyedVectors.load_word2vec_format('../../models/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_lim = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading embeddings\n",
      "loading done\n",
      "SVM\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "def extract_embeddings_as_features_and_gold(conllfile, word_embedding_model, added_features):\n",
    "    '''\n",
    "    Function that extracts features and gold labels using word embeddings\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :param added_features: a boolean specifying whether to include new features or not\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    ### This code was partially inspired by code included in the HLT course, obtained from https://github.com/cltl/ma-hlt-labs/, accessed in May 2020.\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        #check for cases where empty lines mark sentence boundaries (which some conll files do).\n",
    "        if len(row) > 3:\n",
    "            if row[0] in word_embedding_model:\n",
    "                vector = word_embedding_model[row[0]]\n",
    "            else:\n",
    "                vector = [0]*300\n",
    "            features.append(vector)\n",
    "            labels.append(row[-1])\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def extract_features_and_labels(trainingfile, added_features):\n",
    "    data = []\n",
    "    targets = []\n",
    "    # TIP: recall that you can find information on how to integrate features here:\n",
    "    # https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    with open(trainingfile, 'r', encoding='utf8') as infile:\n",
    "        for line in infile:\n",
    "            components = line.rstrip('\\n').split()\n",
    "            if len(components) > 0:\n",
    "                token = components[0]\n",
    "\n",
    "                # added features\n",
    "                if added_features:\n",
    "                    phrase_cat = components[1]\n",
    "                    pos_tag = components[2]\n",
    "\n",
    "                    feature_dict = {'token': token,\n",
    "                                    'cat': phrase_cat,\n",
    "                                    'pos_tag': pos_tag}\n",
    "                else:\n",
    "                    feature_dict = {'token': token}\n",
    "                data.append(feature_dict)\n",
    "                #gold is in the last column\n",
    "                targets.append(components[-1])\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "def extract_features(inputfile, added_features):\n",
    "    data = []\n",
    "    with open(inputfile, 'r', encoding='utf8') as infile:\n",
    "        for line in infile:\n",
    "            components = line.rstrip('\\n').split()\n",
    "            if len(components) > 0:\n",
    "                token = components[0]\n",
    "                # added features\n",
    "                if added_features:\n",
    "                    phrase_cat = components[1]\n",
    "                    pos_tag = components[2]\n",
    "\n",
    "                    feature_dict = {'token': token,\n",
    "                                    'cat': phrase_cat,\n",
    "                                    'pos_tag': pos_tag}\n",
    "                else:\n",
    "                    feature_dict = {'token': token}\n",
    "                data.append(feature_dict)\n",
    "    return data\n",
    "    \n",
    "\n",
    "def create_classifier(train_features, train_targets, modelname, word_to_vec_en):\n",
    "    if modelname ==  'logreg':\n",
    "        model = LogisticRegression(max_iter=iter_lim)\n",
    "        vec = DictVectorizer()\n",
    "        features_vectorized = vec.fit_transform(train_features)\n",
    "        model.fit(features_vectorized, train_targets)\n",
    "\n",
    "    elif modelname == 'NB':\n",
    "        model = MultinomialNB()\n",
    "        vec = DictVectorizer()\n",
    "        features_vectorized = vec.fit_transform(train_features)\n",
    "        model.fit(features_vectorized, train_targets)\n",
    "\n",
    "    elif modelname == 'SVM':\n",
    "        model = SVC(max_iter=iter_lim)\n",
    "        if not word_to_vec_en:\n",
    "            vec = DictVectorizer()\n",
    "            features_vectorized = vec.fit_transform(train_features)\n",
    "        else:\n",
    "            vec = train_features\n",
    "            features_vectorized = train_features\n",
    "\n",
    "        model.fit(features_vectorized, train_targets)\n",
    "\n",
    "    else:\n",
    "        raise Exception()\n",
    "    \n",
    "    return model, vec\n",
    "    \n",
    "    \n",
    "def classify_data(model, vec, inputdata, outputfile, added_features, word_to_vec_en):\n",
    "    if not word_to_vec_en:\n",
    "        features = extract_features(inputdata, added_features)\n",
    "        features = vec.transform(features)\n",
    "    else:\n",
    "        features = vec\n",
    "    predictions = model.predict(features)\n",
    "    outfile = open(outputfile, 'w')\n",
    "    counter = 0\n",
    "    for line in open(inputdata, 'r'):\n",
    "        if len(line.rstrip('\\n').split()) > 0:\n",
    "            outfile.write(line.rstrip('\\n') + '\\t' + predictions[counter] + '\\n')\n",
    "            counter += 1\n",
    "    outfile.close()\n",
    "\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    \n",
    "    #a very basic way for picking up commandline arguments\n",
    "    if argv is None:\n",
    "        argv = sys.argv\n",
    "        \n",
    "    #Note 1: argv[0] is the name of the python program if you run your program as: python program1.py arg1 arg2 arg3\n",
    "    #Note 2: sys.argv is simple, but gets messy if you need it for anything else than basic scenarios with few arguments\n",
    "    #you'll want to move to something better. e.g. argparse (easy to find online)\n",
    "    \n",
    "    \n",
    "    #you can replace the values for these with paths to the appropriate files for now, e.g. by specifying values in argv\n",
    "    #argv = ['mypython_program','','','']\n",
    "    trainingfile = argv[1]\n",
    "    inputfile = argv[2]\n",
    "    outputfile = argv[3]\n",
    "\n",
    "    # Set True to run with added features\n",
    "    added_features = argv[4]\n",
    "    word_to_vec_en = argv[5]\n",
    "    \n",
    "    ## for the word_embedding_model used in the `extract_embeddings_as_features_and_gold' you can either choose to use a statement like this:\n",
    "    if word_to_vec_en:\n",
    "        print('loading embeddings')\n",
    "        language_model = loaded_model\n",
    "        print('loading done')\n",
    "        outputfile = outputfile.replace('.conll','_word2vec.conll')\n",
    "        training_features, gold_labels = extract_embeddings_as_features_and_gold(inputfile, language_model, added_features)\n",
    "    else:\n",
    "        # For now, you can either use word2vec, or added features. Not both.\n",
    "        print('Not Using Embeddings')\n",
    "        training_features, gold_labels = extract_features_and_labels(trainingfile, added_features)\n",
    "    ## and make sure the path works correctly, or you can add an argument to the commandline that allows users to specify the location of the language model.\n",
    "\n",
    "    if added_features:\n",
    "        outputfile = outputfile.replace('.conll','_added_feats.conll')\n",
    "    \n",
    "    for modelname in ['SVM']:\n",
    "        print(modelname)\n",
    "        ml_model, vec = create_classifier(training_features, gold_labels, modelname, word_to_vec_en)\n",
    "        classify_data(ml_model, vec, inputfile, outputfile.replace('.conll','.' + modelname + '.conll'), added_features, word_to_vec_en)\n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # # without added features\n",
    "    # main(['python', \n",
    "    # './data/reuters-train-tab-stripped.en', \n",
    "    # './data/gold_stripped.conll', \n",
    "    # './data/out.conll', False, False])\n",
    "\n",
    "    # # with added features\n",
    "    # main(['python', \n",
    "    # './data/reuters-train-tab-stripped.en',\n",
    "    # './data/gold_stripped.conll', \n",
    "    # './data/out.conll', True, False])\n",
    "    \n",
    "    # with word embeddings and no added features\n",
    "    main(['python', \n",
    "    '../../data/reuters-train-tab-stripped.en', \n",
    "    '../../data/gold_stripped.conll', \n",
    "    '../../data/out.conll', False, True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}